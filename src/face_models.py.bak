#!/usr/bin/env python3

from typing import Dict, Optional, List, Tuple, Union, Any
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import numpy as np
import math  # For kaiming initialization

# List of supported model types
MODEL_TYPES = ['baseline', 'cnn', 'siamese', 'attention', 'arcface', 'hybrid', 'ensemble']

class BaselineNet(nn.Module):
    """Basic CNN model I built for initial testing."""
    def __init__(self, num_classes: int = 18, input_size: Tuple[int, int] = (224, 224)):
        super().__init__()
        # Adding padding=1 to preserve spatial dimensions
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool = nn.MaxPool2d(2, 2)
        
        # Add adaptive pooling to replace manual feature size calculation
        self.adaptive_pool = nn.AdaptiveAvgPool2d(1)
        
        self.fc1 = nn.Linear(128, 512)
        self.fc2 = nn.Linear(512, num_classes)
        self.dropout = nn.Dropout(0.5)  # keeping dropout at 0.5

    def forward(self, x):
        # Apply BatchNorm after convolution and before ReLU
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        
        # Replace manual flattening with adaptive pooling
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

    def get_embedding(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        
        x = F.relu(self.fc1(x))
        return x

class ResNetTransfer(nn.Module):
    """ResNet transfer learning - got much better results with this!"""
    def __init__(self, num_classes: int = 18, freeze_backbone: bool = False):
        super().__init__()
        # Fixed deprecation warning after spending 2 hours debugging
        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        in_feats = self.resnet.fc.in_features
        
        # Add dropout before final FC layer - use much lower dropout rate
        self.dropout = nn.Dropout(0.1)
        
        self.resnet.fc = nn.Sequential(
            self.dropout,
            nn.Linear(in_feats, num_classes)
        )
        
        # Freeze backbone only if explicitly requested (default is now False)
        if freeze_backbone:
            self._freeze_backbone()
    
    def _freeze_backbone(self):
        """Freeze all ResNet layers except the final FC layer"""
        for name, param in self.resnet.named_parameters():
            if "fc" not in name:  # Don't freeze FC layer
                param.requires_grad = False
    
    def unfreeze_backbone(self):
        """Method to unfreeze the backbone for second stage training"""
        for param in self.resnet.parameters():
            param.requires_grad = True

    def forward(self, x):
        # Use the ResNet's built-in forward method
        # This is simpler and less error-prone
        return self.resnet(x)

    def get_embedding(self, x):
        # this extracts features before the final layer
        modules = list(self.resnet.children())[:-1]
        resnet_feats = nn.Sequential(*modules)
        return resnet_feats(x).squeeze()

class SiameseNet(nn.Module):
    # Siamese network implementation based on that CVPR paper
    # Works well when we don't have much data per person
    def __init__(self):
        super().__init__()
        # Modified for 224x224 input images after testing
        self.conv = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4),  # Output: 54x54
            nn.BatchNorm2d(64), 
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2), 
            
            nn.Conv2d(64, 128, kernel_size=5, padding=2), 
            nn.BatchNorm2d(128), 
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2), 
            
            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # Output: 12x12
            nn.BatchNorm2d(256), 
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),  # Output: 6x6
            
            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # Output: 6x6
            nn.BatchNorm2d(512), 
            nn.ReLU(),
        )
        
        # had to calculate this manually - don't mess with these values!
        self.fc = nn.Sequential(
            nn.Linear(512 * 6 * 6, 1024),
            nn.ReLU(),
            nn.Linear(1024, 256)
        )

    def forward_one(self, x):
        feats = self.conv(x)
        feats = feats.view(feats.size(0), -1)  # Flatten
        feats = self.fc(feats)
        # Add L2 normalization to embedding outputs
        feats = F.normalize(feats, p=2, dim=1)
        return feats

    def forward(self, x1, x2):
        out1 = self.forward_one(x1)
        out2 = self.forward_one(x2)
        return out1, out2

    def get_embedding(self, x):
        return self.forward_one(x)

class SpatialAttention(nn.Module):
    """Spatial attention module to complement channel attention"""
    def __init__(self, kernel_size=7):
        super().__init__()
        padding = kernel_size // 2
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # Compute average and max pooling across channel dimension
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        max_pool, _ = torch.max(x, dim=1, keepdim=True)
        
        # Concatenate along the channel dimension
        pooled = torch.cat([avg_pool, max_pool], dim=1)
        
        # Apply convolution and sigmoid activation
        attn_map = self.sigmoid(self.conv(pooled))
        
        # Apply spatial attention to input feature map
        return x * attn_map

class AttentionModule(nn.Module):
    """Self-attention for CNN features - added this after reading that ICCV paper"""
    def __init__(self, in_channels, reduction_ratio=8):
        super().__init__()
        self.query = nn.Conv2d(in_channels, in_channels//reduction_ratio, kernel_size=1)
        self.key = nn.Conv2d(in_channels, in_channels//reduction_ratio, kernel_size=1)
        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))  # learned weight
        self.gamma_value = 0.0  # Monitoring variable for gamma value
        
        # Increase the number of attention heads
        self.num_heads = 2
        self.head_dim = in_channels // (reduction_ratio * self.num_heads)
        
        # Add spatial attention module
        self.spatial_attention = SpatialAttention()
        
    def forward(self, x):
        batch, C, H, W = x.size()
        
        # Project to q, k, v (terminology from the paper)
        q = self.query(x).view(batch, -1, H*W).permute(0, 2, 1)
        k = self.key(x).view(batch, -1, H*W)
        v = self.value(x).view(batch, -1, H*W)
        
        # Calculate attention map - this is the key insight from the paper
        energy = torch.bmm(q, k)
        attention = F.softmax(energy, dim=-1)
        
        # Apply attention to value
        out = torch.bmm(v, attention.permute(0, 2, 1))
        out = out.view(batch, C, H, W)
        
        # Apply channel attention with gamma parameter
        channel_attn_out = self.gamma * out + x
        
        # Store gamma value for monitoring
        self.gamma_value = self.gamma.item()
        
        # Apply spatial attention
        final_out = self.spatial_attention(channel_attn_out)
        
        return final_out

class AttentionNet(nn.Module):
    """ResNet with self-attention - my attempt at improving the model."""
    def __init__(self, num_classes=18):
        super().__init__()
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        # Remove the final FC layer
        self.features = nn.Sequential(*list(self.backbone.children())[:-2])
        # This attention module was a pain to debug but works great now
        self.attention = AttentionModule(512)  # ResNet18 has 512 channels in last layer
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(512, num_classes)
        
    def forward(self, x):
        x = self.features(x)
        x = self.attention(x)
        x = self.gap(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
        
    def get_embedding(self, x):
        x = self.features(x)
        x = self.attention(x)
        x = self.gap(x)
        return x.view(x.size(0), -1)
    
    def get_attention_params(self):
        """Method to monitor attention parameters during training"""
        return {
            "gamma": self.attention.gamma_value
        }

class ArcMarginProduct(nn.Module):
    """ArcFace loss implementation.
    
    """
    def __init__(self, in_feats, out_feats, s=30.0, m=0.5):
        super().__init__()
        self.in_feats = in_feats
        self.out_feats = out_feats
        self.s = s  # scale factor - reduced to match main branch
        self.m = m  # margin - using 0.5 from main branch
        self.weight = nn.Parameter(torch.FloatTensor(out_feats, in_feats))
        # Use Xavier initialization like in main branch
        nn.init.xavier_uniform_(self.weight)
        
    def forward(self, input, label):
        # Normalize features and weights
        x = F.normalize(input)
        w = F.normalize(self.weight)
        
        # Compute cosine similarity
        cos_theta = F.linear(x, w)
        cos_theta = cos_theta.clamp(-1, 1)  # numerical stability
        
        # Add margin to target class
        phi = cos_theta.clone()
        target_mask = torch.zeros_like(cos_theta)
        target_mask.scatter_(1, label.view(-1, 1), 1)
        
        # Apply margin to target class - this is where the magic happens
        phi = torch.where(target_mask.bool(), 
                          torch.cos(torch.acos(cos_theta) + self.m),
                          cos_theta)
        
        # Scale output
        output = phi * self.s
        return output

class ArcFaceNet(nn.Module):
    """Face recognition using ArcFace loss.
    
    """
    def __init__(self, num_classes=18):
        super().__init__()
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        self.features = nn.Sequential(*list(self.backbone.children())[:-1])
        # Simplified embedding layer to match main branch
        self.embedding = nn.Linear(512, 512)  # embedding dimension
        self.arcface = ArcMarginProduct(512, num_classes)
        
    def forward(self, x, labels=None):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        
        # Get embeddings - simplified to match main branch
        emb = self.embedding(x)
        
        if self.training:
            if labels is None:
                # Make it behave like the main branch
                raise ValueError("Labels must be provided during training")
            output = self.arcface(emb, labels)
            return output
        else:
            return emb
            
    def get_embedding(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.embedding(x)

# I tried implementing a transformer block for my presentation
# It's based on "Attention Is All You Need" but modified for vision
# Not sure if it's worth keeping but I'll leave it for now
class TransformerBlock(nn.Module):
    """Simple transformer block for feature refinement."""
    def __init__(self, embed_dim, num_heads=4, ff_dim=2048, dropout=0.1):
        super().__init__()
        # Reduced number of heads from 8 to 4 to match the main branch
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        # Simplify the feed-forward network slightly
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.GELU(),  # GELU works better than ReLU here
            nn.Dropout(dropout),
            nn.Linear(ff_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        # Apply layer normalization before attention (Pre-LN architecture)
        # This often leads to more stable training
        x_norm = self.norm1(x)
        attn_out, _ = self.attention(x_norm, x_norm, x_norm)
        x = x + attn_out  # residual connection
        
        # Apply layer normalization before feed-forward network
        x_norm = self.norm2(x)
        ff_out = self.ff(x_norm)
        x = x + ff_out  # another residual
        
        return x

class HybridNet(nn.Module):
    """My experimental hybrid CNN-Transformer architecture.
    
    This is my attempt at combining traditional CNNs with transformer attention.
    """
    def __init__(self, num_classes=18):
        super().__init__()
        # CNN Feature Extractor - keeping it simple with ResNet18
        self.cnn = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        # Remove classification head
        self.features = nn.Sequential(*list(self.cnn.children())[:-2])
        
        # Feature dimensions
        self.fdim = 512
        self.seq_len = 49  # 7x7 feature map flattened
        
        # Position encoding - crucial for transformers to understand spatial relationships
        self.pos_encoding = nn.Parameter(torch.zeros(self.seq_len, 1, self.fdim))
        nn.init.normal_(self.pos_encoding, mean=0, std=0.02)
        
        # Use a single transformer block like in the main branch - simpler is better
        self.transformer = TransformerBlock(self.fdim)
        
        # Add dropout before final classification
        self.dropout = nn.Dropout(0.1)
        
        # Output layers
        self.norm = nn.LayerNorm(self.fdim)
        self.fc = nn.Linear(self.fdim, num_classes)
    
    def forward(self, x):
        # Extract CNN features
        feats = self.features(x)  # [batch, 512, 7, 7]
        batch_sz = feats.shape[0]
        
        # Reshape for transformer 
        feats = feats.view(batch_sz, self.fdim, -1)  # [batch, 512, 49]
        feats = feats.permute(2, 0, 1)  # [49, batch, 512]
        
        # Add positional encoding
        feats = feats + self.pos_encoding
        
        # Apply transformer directly - simpler approach from main branch
        feats = self.transformer(feats)
        
        # Global pooling (mean) - tried different pooling methods
        feats = feats.mean(dim=0)  # [batch, 512]
        
        # Normalization, dropout, and classification
        feats = self.norm(feats)
        feats = self.dropout(feats)  # Add dropout before final classification
        feats = self.fc(feats)
        
        return feats
        
    def get_embedding(self, x):
        # Pretty much the same as forward but without final classification
        feats = self.features(x)
        batch_sz = feats.shape[0]
        
        feats = feats.view(batch_sz, self.fdim, -1)
        feats = feats.permute(2, 0, 1)
        
        feats = feats + self.pos_encoding
        
        # Apply transformer directly like in main branch
        feats = self.transformer(feats)
        
        feats = feats.mean(dim=0)
        feats = self.norm(feats)
        
        return feats

# Tried both contrastive and triplet loss
# Contrastive worked better in my experiments
class ContrastiveLoss(nn.Module):
    """Loss function for Siamese networks"""
    def __init__(self, margin=2.0):
        super().__init__()
        self.margin = margin  # increased from 1.0 after seeing poor separation
        self.eps = 1e-8  # Small epsilon for numerical stability

    def forward(self, out1, out2, label):
        # Ensure vectors are normalized for more stable distance calculation
        out1 = F.normalize(out1, p=2, dim=1)
        out2 = F.normalize(out2, p=2, dim=1)
        
        # Calculate pairwise distance
        dist = F.pairwise_distance(out1, out2)
        
        # Apply margin more carefully
        dist = torch.clamp(dist, min=self.eps)  # Ensure non-zero distance for stability
        
        # Calculate loss with more robust margin handling
        loss = torch.mean((1-label) * torch.pow(dist, 2) +
                         label * torch.pow(torch.clamp(self.margin - dist, min=0.0), 2))
        return loss

# Function to get the requested model type
def get_model(model_type: str, num_classes: int = 18, input_size: Tuple[int, int] = (224, 224)) -> nn.Module:
    if model_type == 'baseline':
        return BaselineNet(num_classes=num_classes, input_size=input_size)
    elif model_type == 'cnn':
        # Don't freeze backbone by default - allow full training
        return ResNetTransfer(num_classes=num_classes, freeze_backbone=False)
    elif model_type == 'siamese':
        return SiameseNet()
    elif model_type == 'attention':
        return AttentionNet(num_classes=num_classes)
    elif model_type == 'arcface':
        return ArcFaceNet(num_classes=num_classes)
    elif model_type == 'hybrid':
        return HybridNet(num_classes=num_classes)
    elif model_type == 'ensemble':
        # Default ensemble combines CNN, AttentionNet, and ArcFace models
        return create_ensemble(['cnn', 'attention', 'arcface'], num_classes=num_classes)
    elif isinstance(model_type, list):
        # If a list of model types is provided, create an ensemble
        return create_ensemble(model_type, num_classes=num_classes)
    else:
        raise ValueError(f"Invalid model type: {model_type}")

def get_criterion(model_type: str) -> nn.Module:
    if model_type in ['baseline', 'cnn', 'attention', 'hybrid', 'ensemble']:
        return nn.CrossEntropyLoss()
    elif model_type == 'siamese':
        return ContrastiveLoss()
    elif model_type == 'arcface':
        # ArcFace handles loss internally - this confused me at first
        return nn.CrossEntropyLoss()
    else:
        raise ValueError(f"Invalid model type: {model_type}")

# Old implementation I tried first - keeping for reference
# def get_model_v1(model_type, num_classes):
#     if model_type == 'baseline':
#         return BaselineNet(num_classes)
#     elif model_type == 'cnn':
#         model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
#         model.fc = nn.Linear(model.fc.in_features, num_classes)
#         return model
#     else:
#         raise ValueError(f"Unknown model: {model_type}") 

class EnsembleModel(nn.Module):
    """Ensemble model that combines predictions from multiple face recognition models."""
    def __init__(self, models: List[nn.Module], ensemble_method: str = 'average'):
        """Initialize ensemble model.
        
        Args:
            models: List of models to ensemble
            ensemble_method: Method to combine predictions ('average', 'weighted', 'max')
        """
        super().__init__()
        self.models = nn.ModuleList(models)
        self.ensemble_method = ensemble_method
        self.weights = nn.Parameter(torch.ones(len(models)) / len(models), requires_grad=(ensemble_method == 'weighted'))
    
    def forward(self, x):
        """Forward pass through all models and combine predictions."""
        # Get outputs from all models
        outputs = []
        for model in self.models:
            if hasattr(model, 'training') and model.training:
                model.eval()  # Always use eval mode for ensemble
                
            # Handle different model types
            if isinstance(model, ArcFaceNet):
                # For ArcFace, we need to extract embeddings and compute logits
                embeddings = model(x)
                logits = F.linear(F.normalize(embeddings), F.normalize(model.arcface.weight))
                outputs.append(logits)
            elif isinstance(model, SiameseNet):
                # For SiameseNet, we need reference embeddings and the inference is different
                # come back later to implement this if i have time
                continue
            else:
                # For standard classification models
                outputs.append(model(x))
        
        # Skip ensemble if only one valid model
        if len(outputs) == 1:
            return outputs[0]
        
        # Combine predictions based on ensemble method
        if self.ensemble_method == 'average':
            return torch.mean(torch.stack(outputs), dim=0)
        elif self.ensemble_method == 'weighted':
            normalized_weights = F.softmax(self.weights, dim=0)
            weighted_outputs = torch.stack([normalized_weights[i] * outputs[i] for i in range(len(outputs))])
            return torch.sum(weighted_outputs, dim=0)
        elif self.ensemble_method == 'max':
            # Convert to probabilities and take max
            probs = [F.softmax(output, dim=1) for output in outputs]
            max_probs, _ = torch.max(torch.stack(probs), dim=0)
            # Convert back to logits for loss computation
            return torch.log(max_probs)
        else:
            raise ValueError(f"Unknown ensemble method: {self.ensemble_method}")
    
    def get_embedding(self, x):
        """Get combined embeddings from all models."""
        # Collect embeddings from all models
        embeddings = []
        for model in self.models:
            if hasattr(model, 'get_embedding'):
                emb = model.get_embedding(x)
                embeddings.append(emb)
        
        # Handle the embeddings
        if len(embeddings) > 1:
            # Concatenate embeddings if multiple models provided embeddings
            return torch.cat(embeddings, dim=1)
        elif len(embeddings) == 1:
            # Just return the single embedding
            return embeddings[0]
        else:
            # No embeddings collected
            return None

def create_ensemble(model_types: List[str], num_classes: int, ensemble_method: str = 'average') -> EnsembleModel:
    """Create an ensemble model from multiple model architectures.
    
    Args:
        model_types: List of model type strings
        num_classes: Number of classes for classification
        ensemble_method: Method to combine predictions
        
    Returns:
        Ensemble model
    """
    models = []
    for model_type in model_types:
        model = get_model(model_type, num_classes=num_classes)
        models.append(model)
    
    return EnsembleModel(models, ensemble_method=ensemble_method) 